{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03cfecd",
   "metadata": {},
   "source": [
    "# NLP Final Project Interim Report\n",
    "\n",
    "#### Peter Charles Bailey (Charlie)\n",
    "#### CU ID: peba2926"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457a2ef",
   "metadata": {},
   "source": [
    "### Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b977702",
   "metadata": {},
   "source": [
    "The goal of this project is to address the challenges of understanding the nuance of human emotion as it is conveyed through text. By fine-tuning a BERT model to classify texted based on six core human emotions motions (Sadness, Joy, Love, Anger, Fear, Surprise), this project aims to improve emotion detection accuracy in written communication—such as social media posts.\n",
    "\n",
    "This project is particularly important as our world becomes increasingly digital and more polarized. By improving the recognition of emotional cues in text, a system such as the one proposed can potentially help bridge digital communication gaps and prevent conflict arising from simple misinterpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2e888",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4567c9ea",
   "metadata": {},
   "source": [
    "#### Description\n",
    "For this project, I will be using the [**dair-ai/emotion**](https://huggingface.co/datasets/dair-ai/emotion) dataset from Hugging Face. There are two configurations of this dataset available—a split and unsplit version.\n",
    "\n",
    "| Name | Train | Validation | Test |\n",
    "|------|-------|------------|------|\n",
    "| split | 16,000 | 2,000 | 2,000 |\n",
    "| unsplit | 416,809 | N/A | N/A |\n",
    "\n",
    "To start, I plan to do some exploration with the `split` configuration, then scale up to do a full run on the `unsplit` version.\n",
    "\n",
    "Both versions have two features:\n",
    "\n",
    "1. a text string derived from English Twitter messages\n",
    "2. a numeric classification label with the following possible values: `sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5)`\n",
    "\n",
    "As noted above, the `split` configuration has 16,000 train instances, 2,000 validation instances,and 2,000 test instances—for a total of 20,000 instances overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08389e0b",
   "metadata": {},
   "source": [
    "#### EDA\n",
    "I have done some basic exploration on the `split` dataset. I confirmed that all of the instance counts are correct for each given set (train, validation, and test) and did a check for any missing or duplicate values. I then did a histogram plot of the class label distributions. As seen in the histogram in the Progress So Far section, there are some significant class imbalances in this dataset. These will need to be accounted for in the model creation and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84651f7e",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f960650c",
   "metadata": {},
   "source": [
    "#### Overall\n",
    "My current plan is to do EDA and model prototyping in a Jupyter Notebook on the `split` version of this dataset. Once I have a good baseline model working, I will scale up and test my architecture on the full `unsplit` dataset. After I confirm that my architecture can handle the full dataset in a reasonable amount of training time, I will begin iterating between architecture improvements (hyperparameter tuning) at the smaller scale, and load testing on the full dataset. Once I have an end-to-end system I am happy with, I will move the code into a stand-alone Python program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115d870",
   "metadata": {},
   "source": [
    "#### Tech Stack\n",
    "* **PyTorch** framework for running the BERT model and providing integration with my local GPUs.\n",
    "* **Transformers** library from Hugging Face to utilize the pre-trained BERT models for fine-tuning.\n",
    "* **Datasets** library from Hugging Face for loading the data into my program.\n",
    "* **Pandas** library for EDA and data manipulation.\n",
    "* **Seaborn/Matplotlib** libraries for generating data visualizations.\n",
    "* **NumPy** package for dealing with vectorized data.\n",
    "* **Sklearn** library for model evaluation statistics.\n",
    "* **tqdm** library to easily visualize model training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c1e91",
   "metadata": {},
   "source": [
    "### Progress So Far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b50dd",
   "metadata": {},
   "source": [
    "So far, I have completed the initial data loading, cleaning, and simple EDA. For the most part, this dataset is cleaned, in the right format, and ready for processing. There is a single duplicate value, but given the size of the dataset, this will have in indistinguishable impact on performance, so I'm planning to leave it.\n",
    "\n",
    "One important point I've noted are the class imbalances.\n",
    "\n",
    "![Emotion Class Distribution](../assets/emotion_class_hist.png)\n",
    "\n",
    "While we see that there is a similar distribution pattern across all three datasets, within each dataset, classes 0 and 1 are heavily overrepresented and class 5 in particular is very underrepresented. These imbalances will need to be accounted for in building and evaluating the BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87e9de",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6fd371",
   "metadata": {},
   "source": [
    "Here are the immediate next steps that I am currently WIP on:\n",
    "\n",
    "1. Determine which BERT model I want to use as my baseline.\n",
    "2. Determine the initial hyperparameters I want to use.\n",
    "3. Figure out how to handle the class imbalances.\n",
    "4. Build and train the initial architecture (tokenize -> setup model -> train).\n",
    "5. Evaluate the model (F1, confusion matrix).\n",
    "\n",
    "After these immediate steps are complete, here are the medium term steps I plan to complete following my high-level approach laid out above:\n",
    "\n",
    "6. Iterate steps 4 and 5 to get an improved baseline model.\n",
    "7. Test this improved model on the larger `unsplit` dataset.\n",
    "8. If time permits, iterate again between steps 6 and 7 to get a final model.\n",
    "9. Convert this final system into a stand alone Python program.\n",
    "10. (Really stretch goal) Setup a simple Flask app that allows text to be entered and analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb818d1",
   "metadata": {},
   "source": [
    "### Questions or Concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51022432",
   "metadata": {},
   "source": [
    "Looking ahead, here are the biggest areas of the project that I have questions / concerns about:\n",
    "\n",
    "1. Is there a BERT base model that is recommended for emotion classification?\n",
    "2. How are the initial hyperparameters set for fine-tuning? In general, is there a systematic way to determine the hyperparameters?\n",
    "3. Building on the point above, what does the process of hyperparameter tuning look like with these models? Is there something akin to GridSearch for transformer models?\n",
    "4. What are the best practices for dealing with class imbalances of the type I have in my dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 (NLP ENV)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
