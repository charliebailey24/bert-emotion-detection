{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project Related Work\n",
    "\n",
    "#### Peter Charles Bailey (Charlie)\n",
    "#### CU ID: peba2926"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Detection in Text\n",
    "\n",
    "Emotion is one of the crucial human feedback mechanisms that is difficult to transmit with it's full potential in the written form. This project aims to fine-tune a **BERT** model to detect and distinguish between the 6 universally recognized human emotions of **Anger, Disgust, Fear, Joy, Sadness, Surprise**. Using a dataset such as the **dair-ai/emotion** dataset, the model will be trained to detect emotions in posts such as those on social media sites like **X**.\n",
    "\n",
    "To evaluate the model's effectiveness, we will measure the **accuracy** and **F1 scores** on a test dataset compared against a ground truth emotion-labeled dataset.\n",
    "\n",
    "In our increasingly digital world, being able to accurately detect emotion in text is crucial for effective communication. Exploring the ability of transformer-based models to help humans better communicate via the written medium opens the door to numerous potential use cases that could help people better understand each other in an increasingly divided world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Work\n",
    "\n",
    "#### 1. Transformer models for text-based emotion detection: a review of BERT-based approaches\n",
    "\n",
    "##### Citation\n",
    "Acheampong, F. A., Nunoo-Mensah, H., & Chen, W. (2021). [Transformer models for text-based emotion detection: A review of BERT-based approaches](https://link.springer.com/article/10.1007/s10462-021-09958-2). Artificial Intelligence Review, 54(8), 5789–5829.\n",
    "\n",
    "##### Summary\n",
    "This paper, authored in 2021, offers a comprehensive survey of the most recent advances in emotion recognition in text, with an emphasis on BERT architectures. This paper is robust survey that includes the architecture, datasets, contributions, results, and limitations of fifteen different research projects. One of the works that stood out was Huang et al. (2019a) which proposed a contextual emotion classifier (EmotionX-KU). Their model consisted of three main phases: casual utterance modeling, model pre-training using BERT, and fine-tuning. They used the EmotionLines and Friends datasets and obtained micro-F1 scores of 81.5 and 88.5 on each dataset respectively.\n",
    "\n",
    "##### Key Takeaways\n",
    "Overall, this paper was a great introduction to the current state of the art in how transformer models (particularly BERT) are being used to detect emotion in text. It highlighted the various datasets that are available, gave a good overview of the proposed architectures (some are quite complex), and provided a good reference on the evaluation metrics being used. One particular takeaway that I plan to integrate into my project is the use of a micro-F1 score for evaluation. This is a variation of the F1 score that is useful in evaluating multi-class classification models (such as emotion detection). One other key takeaway was the mathematical definition of text based emotion detection. From p. 5:\n",
    "\n",
    "Defining text based emotions can be done mathematically as,\n",
    "\n",
    "$$ r: A * T \\rightarrow E $$\n",
    "\n",
    "where $A$ is an author that writes down a specific text, $T$ is the written text from which emotions are to be extracted, and $r$ is the relationship between the author and their written texts.\n",
    "\n",
    "It is useful to have a concrete mathematical model to go off of in terms of defining the task I am seeking to accomplish in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "\n",
    "##### Citation\n",
    "Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_m9bbH_7ECE1h3lZ3D61TYg52rKpifVNjL4fvJ85uqggrXsWDBTB7YooFLJeNXHWqhvOyC). In Proceedings of NAACL-HLT 2019, pages 4171–4186.\n",
    "\n",
    "##### Summary\n",
    "This is the original paper that proposes the BERT (Bidirectional Encoder Representation from Transformers) architecture for natural language processing. While this doesn't specifically address the project goal of textual emotion detection, I felt it was important to incorporate for context, given how prominent the BERT architecture is in emotion classification. This paper demonstrates that the BERT architecture has achieved state of the art results on sentiment analysis tasks, which makes it a good candidate architecture for emotion detection.\n",
    "\n",
    "##### Key Takeaways\n",
    "One major takeaway from this paper in terms of application to this research project is understanding the bidirectional nature of BERT. Where a GPT architecture works in a left-to-right manner, it can only attend to the context on it's left, BERT uses the context on both the left and right simultaneously. This is an important aspect for emotion detection as emotion cues can depend on the full context of a given sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. EmoDet2: Emotion Detection in English Textual Dialogue using BERT and BiLSTM Models\n",
    "\n",
    "##### Citation\n",
    "Al-Omari, H., Abdullah, M. A., & Shaikh, S. (2020). [EmoDet2: Emotion detection in English textual dialogue using BERT and BiLSTM models](https://ieeexplore.ieee.org/abstract/document/9078946). In 2020 11th International Conference on Information and Communication Systems (ICICS).\n",
    "\n",
    "##### Summary\n",
    "This paper presents the EmoDet2 system for emotion detection. It is a coarse-grained system the classifies text into four categories: happy, sad, angry and other. Overall, the system is somewhat complex. It is an ensemble approach that uses different sub-models: a neural network, two BiLSTMS, and two BERT-BiLSTM models. With all of this complexity, though, they are able to achieve an F1-score of 0.75, which surpassed the competition baseline (F1-score 0.58) and shows that this approach is promising.\n",
    "\n",
    "##### Key Takeaways\n",
    "Overall, I found this paper to be a great deep dive into the architecture and design decisions that go into a more complex emotion detection system. While this approach is too complex for the implementation I will be pursuing, I still think there will be aspects of this work that I will be able to incorporate into my implementation. I am particularly intrigued by the detailed exploration of hyperparameter tuning the authors performed—as well as the ensembling method used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. GoEmotions: A Dataset of Fine-Grained Emotions\n",
    "\n",
    "##### Citation\n",
    "Demszky, D., Movshovitz-Attias, D., Ko, J., Cowen, A., Nemade, G., & Ravi, S. (2020). [GoEmotions: A Dataset of Fine-Grained Emotions](https://arxiv.org/abs/2005.00547). In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).\n",
    "\n",
    "##### Summary\n",
    "This paper presents the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. The authors took great care in ensuring the quality of the data by filtering and correcting (as much as possible) for the selection bias that comes with sourcing data from Reddit. They were as meticulous about the data labeling procedure and had all ratings performed manually by human raters. In addition to aggregating this massive dataset, they also fine-tuned a BERT model to serve as a baseline comparison for future research. Their model achieved a 46% macro-F1 score across the 28 emotion labels. On the one hand, the authors note that this outperforms earlier methods—validating the use of the BERT architecture for emotion detection—but also leave significant room for future improvement.\n",
    "\n",
    "##### Key Takeaways\n",
    "One of the biggest key takeaways from this paper was finding a potential dataset to use for this project. I found the authors methodology for collecting the data to be quite rigorous. In addition, their BERT model architecture is straight forward and should be relatively easy to replicate and improve upon. Overall, I feel this project serves as a very good jumping off point for my own experimentation. It also offers a very clear benchmark to compare against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. BERT-CNN: A deep learning model for detecting emotions from text\n",
    "\n",
    "##### Citation\n",
    "Abas, A. R., Elhenawy, I., Zidan, M., & Othman, M. (2022). [BERT-CNN: A deep learning model for detecting emotions from text](https://d1wqtxts1xzle7.cloudfront.net/85738797/pdf-libre.pdf?1652091190=&response-content-disposition=inline%3B+filename%3DBERT_CNN_A_Deep_Learning_Model_for_Detec.pdf&Expires=1744180629&Signature=UXud-qMbxNpYHjHOiId2XqtFkrNAhLrBGdhcG5LKcP6D8~WjkzCT1ljTXHrqS1EvjO8KHbhZoRj~J1s8zUhvV4Di7lq8KGyvvkgCnFtlp4w7QKFF0xyxz7YYBVT7itvkdk4MHeBPQDpHtTv8wtDHmjv8DTjfbkS7puzZVgAJxX6Yo9ejzwhiBD6U3vbjc9uPu1J~V8YTqbISHFNNyQq7K8ZeY9zy5nxxaFYsL6gxzeQZXhpi30bJ6tvEN~XtEvMsK1T6n7VsQRNZ~HFssBPfTzXnX7i1~XtWxpOD72WVBLqk10AkzARS~N98Y5dFlz6StWGJtftBec2Nuo5L5nYMZw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA). Computers, Materials & Continua, 71(2), 2943–2961\n",
    "\n",
    "##### Summary\n",
    "In this paper the authors present a new model for detecting emotions in text called BERT-CNN. As the name implies, this model combines a BERT model with a Convolutional Neural Network (CNN). The authors tested their model using the semeval 2019 task3 dataset and ISEAR datasets. The model achieves an accuracy of 94.7% and an F1-score of 94% for semeval 2019 task3 dataset and an accuracy of 75.8% and an F1-score of 76% for ISEAR dataset. This significantly outperforms the baseline competition on these datasets.\n",
    "\n",
    "##### Key Takeaways\n",
    "Of all the architectures presented thus far, this is probably the one I am most excited about. At a high-level, the reasoning for why this model works is easy to grasp—the BERT model handles the language understanding at a coarse-grain level, then the CNN does the fine-grained emotion classification. Once I have a baseline model up and running, this is the improvement architecture I would like to try an replicate in my project."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
